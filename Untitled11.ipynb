{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AUr8TvNClm7O7pQzz_v2OSEmKX3dKmyy",
      "authorship_tag": "ABX9TyMm0URTgi4e0dtGnhuWk9s0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iramiramiramiramiram/Brain-Tumour-Localisation-and-Classification/blob/main/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/mitacs_proj\n",
        "!touch ~/mitacs_proj/abc.json"
      ],
      "metadata": {
        "id": "MFfSlQm_noeF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/MITACS_PROJECT/output_models.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnMxebL5nrYo",
        "outputId": "d2146da2-03b7-4f5e-9f1f-f6e52723ab29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/MITACS_PROJECT/output_models.zip\n",
            "  inflating: tumor_detector_axial.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/MITACS_PROJECT/yolov5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC9ZkRRonu_g",
        "outputId": "bdbf1cf6-3565-4f50-87d1-34e48d538e74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/MITACS_PROJECT/yolov5.zip\n",
            "   creating: .git/\n",
            "   creating: .github/\n",
            "   creating: __pycache__/\n",
            "   creating: classify/\n",
            "   creating: data/\n",
            "   creating: models/\n",
            "   creating: runs/\n",
            "   creating: segment/\n",
            "   creating: utils/\n",
            "  inflating: .gitignore              \n",
            "  inflating: tutorial.ipynb          \n",
            "  inflating: CITATION.cff            \n",
            "  inflating: benchmarks.py           \n",
            "  inflating: setup.cfg               \n",
            "  inflating: README.zh-CN.md         \n",
            "  inflating: hubconf.py              \n",
            "  inflating: .pre-commit-config.yaml  \n",
            "  inflating: export.py               \n",
            "  inflating: train.py                \n",
            "  inflating: val.py                  \n",
            "  inflating: requirements.txt        \n",
            "  inflating: detect.py               \n",
            "  inflating: CONTRIBUTING.md         \n",
            "  inflating: .dockerignore           \n",
            "  inflating: .gitattributes          \n",
            "  inflating: LICENSE                 \n",
            "  inflating: README.md               \n",
            "   creating: .github/ISSUE_TEMPLATE/\n",
            "   creating: .github/workflows/\n",
            "  inflating: .github/PULL_REQUEST_TEMPLATE.md  \n",
            "  inflating: .github/dependabot.yml  \n",
            "  inflating: .github/ISSUE_TEMPLATE/feature-request.yml  \n",
            "  inflating: .github/ISSUE_TEMPLATE/bug-report.yml  \n",
            "  inflating: .github/ISSUE_TEMPLATE/config.yml  \n",
            "  inflating: .github/ISSUE_TEMPLATE/question.yml  \n",
            "  inflating: .github/workflows/ci-testing.yml  \n",
            "  inflating: .github/workflows/greetings.yml  \n",
            "  inflating: .github/workflows/stale.yml  \n",
            "  inflating: .github/workflows/translate-readme.yml  \n",
            "  inflating: .github/workflows/codeql-analysis.yml  \n",
            "  inflating: .github/workflows/docker.yml  \n",
            "  inflating: .github/workflows/links.yml  \n",
            "   creating: .git/branches/\n",
            "   creating: .git/hooks/\n",
            "   creating: .git/info/\n",
            "   creating: .git/logs/\n",
            "   creating: .git/objects/\n",
            "   creating: .git/refs/\n",
            "  inflating: .git/description        \n",
            "  inflating: .git/config             \n",
            "  inflating: .git/index              \n",
            "  inflating: .git/FETCH_HEAD         \n",
            "  inflating: .git/HEAD               \n",
            "  inflating: .git/packed-refs        \n",
            "  inflating: .git/info/exclude       \n",
            "   creating: .git/refs/heads/\n",
            "   creating: .git/refs/remotes/\n",
            "   creating: .git/refs/tags/\n",
            "   creating: .git/refs/remotes/origin/\n",
            "  inflating: .git/refs/remotes/origin/HEAD  \n",
            "  inflating: .git/refs/heads/master  \n",
            "   creating: .git/objects/info/\n",
            "   creating: .git/objects/pack/\n",
            "  inflating: .git/objects/pack/pack-114c7b58fa77f649da8cb451bdef412598ac538f.idx  \n",
            "  inflating: .git/objects/pack/pack-114c7b58fa77f649da8cb451bdef412598ac538f.pack  \n",
            "   creating: .git/logs/refs/\n",
            "  inflating: .git/logs/HEAD          \n",
            "   creating: .git/logs/refs/heads/\n",
            "   creating: .git/logs/refs/remotes/\n",
            "   creating: .git/logs/refs/remotes/origin/\n",
            "  inflating: .git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: .git/logs/refs/heads/master  \n",
            "  inflating: .git/hooks/push-to-checkout.sample  \n",
            "  inflating: .git/hooks/pre-rebase.sample  \n",
            "  inflating: .git/hooks/post-update.sample  \n",
            "  inflating: .git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: .git/hooks/pre-commit.sample  \n",
            "  inflating: .git/hooks/commit-msg.sample  \n",
            "  inflating: .git/hooks/pre-applypatch.sample  \n",
            "  inflating: .git/hooks/update.sample  \n",
            "  inflating: .git/hooks/pre-push.sample  \n",
            "  inflating: .git/hooks/pre-receive.sample  \n",
            "  inflating: .git/hooks/applypatch-msg.sample  \n",
            "  inflating: .git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: .git/hooks/pre-merge-commit.sample  \n",
            "  inflating: classify/tutorial.ipynb  \n",
            "  inflating: classify/train.py       \n",
            "  inflating: classify/val.py         \n",
            "  inflating: classify/predict.py     \n",
            "  inflating: __pycache__/export.cpython-310.pyc  \n",
            "  inflating: __pycache__/val.cpython-310.pyc  \n",
            "   creating: runs/detect/\n",
            "   creating: runs/train/\n",
            "   creating: runs/detect/exp/\n",
            "   creating: runs/detect/exp/labels/\n",
            "  inflating: runs/detect/exp/00018_126.jpg  \n",
            "  inflating: runs/detect/exp/labels/00018_126.txt  \n",
            "   creating: runs/train/axial/\n",
            "   creating: runs/train/axial/weights/\n",
            "  inflating: runs/train/axial/hyp.yaml  \n",
            "  inflating: runs/train/axial/labels_correlogram.jpg  \n",
            "  inflating: runs/train/axial/F1_curve.png  \n",
            "  inflating: runs/train/axial/labels.jpg  \n",
            "  inflating: runs/train/axial/opt.yaml  \n",
            "  inflating: runs/train/axial/results.csv  \n",
            "  inflating: runs/train/axial/results.png  \n",
            "  inflating: runs/train/axial/events.out.tfevents.1696592317.89d67c90b098.8317.0  \n",
            "  inflating: runs/train/axial/P_curve.png  \n",
            "  inflating: runs/train/axial/PR_curve.png  \n",
            "  inflating: runs/train/axial/train_batch0.jpg  \n",
            "  inflating: runs/train/axial/R_curve.png  \n",
            "  inflating: runs/train/axial/train_batch1.jpg  \n",
            "  inflating: runs/train/axial/val_batch0_pred.jpg  \n",
            "  inflating: runs/train/axial/train_batch2.jpg  \n",
            "  inflating: runs/train/axial/val_batch0_labels.jpg  \n",
            "  inflating: runs/train/axial/confusion_matrix.png  \n",
            "  inflating: runs/train/axial/weights/last.pt  \n",
            "  inflating: runs/train/axial/weights/best.pt  \n",
            "  inflating: segment/tutorial.ipynb  \n",
            "  inflating: segment/train.py        \n",
            "  inflating: segment/val.py          \n",
            "  inflating: segment/predict.py      \n",
            "   creating: models/__pycache__/\n",
            "   creating: models/hub/\n",
            "   creating: models/segment/\n",
            "  inflating: models/yolov5l.yaml     \n",
            "  inflating: models/tf.py            \n",
            "  inflating: models/yolov5x.yaml     \n",
            "  inflating: models/yolov5s.yaml     \n",
            "  inflating: models/experimental.py  \n",
            "  inflating: models/yolov5n.yaml     \n",
            "  inflating: models/__init__.py      \n",
            "  inflating: models/yolo.py          \n",
            "  inflating: models/yolov5m.yaml     \n",
            "  inflating: models/common.py        \n",
            "  inflating: models/__pycache__/experimental.cpython-310.pyc  \n",
            "  inflating: models/__pycache__/common.cpython-310.pyc  \n",
            "  inflating: models/__pycache__/yolo.cpython-310.pyc  \n",
            "  inflating: models/__pycache__/__init__.cpython-310.pyc  \n",
            "  inflating: models/segment/yolov5x-seg.yaml  \n",
            "  inflating: models/segment/yolov5n-seg.yaml  \n",
            "  inflating: models/segment/yolov5l-seg.yaml  \n",
            "  inflating: models/segment/yolov5s-seg.yaml  \n",
            "  inflating: models/segment/yolov5m-seg.yaml  \n",
            "  inflating: models/hub/yolov5l6.yaml  \n",
            "  inflating: models/hub/anchors.yaml  \n",
            "  inflating: models/hub/yolov5n6.yaml  \n",
            "  inflating: models/hub/yolov5-p7.yaml  \n",
            "  inflating: models/hub/yolov5-p6.yaml  \n",
            "  inflating: models/hub/yolov5-fpn.yaml  \n",
            "  inflating: models/hub/yolov5x6.yaml  \n",
            "  inflating: models/hub/yolov3-tiny.yaml  \n",
            "  inflating: models/hub/yolov5s6.yaml  \n",
            "  inflating: models/hub/yolov5-p34.yaml  \n",
            "  inflating: models/hub/yolov5m6.yaml  \n",
            "  inflating: models/hub/yolov5s-transformer.yaml  \n",
            "  inflating: models/hub/yolov3.yaml  \n",
            "  inflating: models/hub/yolov5-p2.yaml  \n",
            "  inflating: models/hub/yolov5-bifpn.yaml  \n",
            "  inflating: models/hub/yolov3-spp.yaml  \n",
            "  inflating: models/hub/yolov5s-LeakyReLU.yaml  \n",
            "  inflating: models/hub/yolov5s-ghost.yaml  \n",
            "  inflating: models/hub/yolov5-panet.yaml  \n",
            "   creating: utils/__pycache__/\n",
            "   creating: utils/aws/\n",
            "   creating: utils/docker/\n",
            "   creating: utils/flask_rest_api/\n",
            "   creating: utils/google_app_engine/\n",
            "   creating: utils/loggers/\n",
            "   creating: utils/segment/\n",
            "  inflating: utils/callbacks.py      \n",
            "  inflating: utils/augmentations.py  \n",
            "  inflating: utils/general.py        \n",
            "  inflating: utils/torch_utils.py    \n",
            "  inflating: utils/dataloaders.py    \n",
            "  inflating: utils/plots.py          \n",
            "  inflating: utils/autobatch.py      \n",
            "  inflating: utils/loss.py           \n",
            "  inflating: utils/activations.py    \n",
            "  inflating: utils/__init__.py       \n",
            "  inflating: utils/downloads.py      \n",
            "  inflating: utils/triton.py         \n",
            "  inflating: utils/autoanchor.py     \n",
            "  inflating: utils/metrics.py        \n",
            "  inflating: utils/flask_rest_api/example_request.py  \n",
            "  inflating: utils/flask_rest_api/restapi.py  \n",
            "  inflating: utils/flask_rest_api/README.md  \n",
            "  inflating: utils/aws/resume.py     \n",
            "  inflating: utils/aws/__init__.py   \n",
            "  inflating: utils/aws/mime.sh       \n",
            "  inflating: utils/aws/userdata.sh   \n",
            "  inflating: utils/google_app_engine/Dockerfile  \n",
            "  inflating: utils/google_app_engine/additional_requirements.txt  \n",
            "  inflating: utils/google_app_engine/app.yaml  \n",
            "  inflating: utils/__pycache__/autoanchor.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/callbacks.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/metrics.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/dataloaders.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/augmentations.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/torch_utils.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/loss.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/downloads.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/autobatch.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/__init__.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/general.cpython-310.pyc  \n",
            "  inflating: utils/__pycache__/plots.cpython-310.pyc  \n",
            "  inflating: utils/docker/Dockerfile  \n",
            "  inflating: utils/docker/Dockerfile-cpu  \n",
            "  inflating: utils/docker/Dockerfile-arm64  \n",
            "  inflating: utils/segment/augmentations.py  \n",
            "  inflating: utils/segment/general.py  \n",
            "  inflating: utils/segment/dataloaders.py  \n",
            "  inflating: utils/segment/plots.py  \n",
            "  inflating: utils/segment/loss.py   \n",
            "  inflating: utils/segment/__init__.py  \n",
            "  inflating: utils/segment/metrics.py  \n",
            "   creating: utils/loggers/__pycache__/\n",
            "   creating: utils/loggers/clearml/\n",
            "   creating: utils/loggers/comet/\n",
            "   creating: utils/loggers/wandb/\n",
            "  inflating: utils/loggers/__init__.py  \n",
            "   creating: utils/loggers/comet/__pycache__/\n",
            "  inflating: utils/loggers/comet/comet_utils.py  \n",
            "  inflating: utils/loggers/comet/optimizer_config.json  \n",
            "  inflating: utils/loggers/comet/hpo.py  \n",
            "  inflating: utils/loggers/comet/__init__.py  \n",
            "  inflating: utils/loggers/comet/README.md  \n",
            "  inflating: utils/loggers/comet/__pycache__/comet_utils.cpython-310.pyc  \n",
            "  inflating: utils/loggers/comet/__pycache__/__init__.cpython-310.pyc  \n",
            "  inflating: utils/loggers/__pycache__/__init__.cpython-310.pyc  \n",
            "   creating: utils/loggers/wandb/__pycache__/\n",
            "  inflating: utils/loggers/wandb/__init__.py  \n",
            "  inflating: utils/loggers/wandb/wandb_utils.py  \n",
            "  inflating: utils/loggers/wandb/__pycache__/wandb_utils.cpython-310.pyc  \n",
            "  inflating: utils/loggers/wandb/__pycache__/__init__.cpython-310.pyc  \n",
            "   creating: utils/loggers/clearml/__pycache__/\n",
            "  inflating: utils/loggers/clearml/hpo.py  \n",
            "  inflating: utils/loggers/clearml/__init__.py  \n",
            "  inflating: utils/loggers/clearml/clearml_utils.py  \n",
            "  inflating: utils/loggers/clearml/README.md  \n",
            "  inflating: utils/loggers/clearml/__pycache__/clearml_utils.cpython-310.pyc  \n",
            "  inflating: utils/loggers/clearml/__pycache__/__init__.cpython-310.pyc  \n",
            "   creating: data/hyps/\n",
            "   creating: data/images/\n",
            "   creating: data/scripts/\n",
            "  inflating: data/VOC.yaml           \n",
            "  inflating: data/GlobalWheat2020.yaml  \n",
            "  inflating: data/xView.yaml         \n",
            "  inflating: data/ImageNet.yaml      \n",
            "  inflating: data/Objects365.yaml    \n",
            "  inflating: data/coco128.yaml       \n",
            "  inflating: data/coco.yaml          \n",
            "  inflating: data/coco128-seg.yaml   \n",
            "  inflating: data/VisDrone.yaml      \n",
            "  inflating: data/SKU-110K.yaml      \n",
            "  inflating: data/Argoverse.yaml     \n",
            "  inflating: data/images/zidane.jpg  \n",
            "  inflating: data/images/bus.jpg     \n",
            "  inflating: data/hyps/hyp.VOC.yaml  \n",
            "  inflating: data/hyps/hyp.scratch-med.yaml  \n",
            "  inflating: data/hyps/hyp.no-augmentation.yaml  \n",
            "  inflating: data/hyps/hyp.scratch-low.yaml  \n",
            "  inflating: data/hyps/hyp.scratch-high.yaml  \n",
            "  inflating: data/hyps/hyp.Objects365.yaml  \n",
            "  inflating: data/scripts/download_weights.sh  \n",
            "  inflating: data/scripts/get_coco.sh  \n",
            "  inflating: data/scripts/get_coco128.sh  \n",
            "  inflating: data/scripts/get_imagenet.sh  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vfTHTGdfcb7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yolo_model(model_path):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=True).to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "vqktqlJLnHTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def extract_features_from_mri(yolo_output):\n",
        "    bbox_areas = []\n",
        "    # We assume that 'yolo_output' is a tensor where each row represents a detection.\n",
        "    # A row contains [x1, y1, x2, y2, confidence, class]\n",
        "    for det in yolo_output:\n",
        "        if det[4] >= 0.25:  # confidence threshold\n",
        "            bbox_area = (det[2] - det[0]) * (det[3] - det[1])  # calculates the area of the bounding box\n",
        "            bbox_areas.append(bbox_area.item())\n",
        "    features = {'bbox_areas': bbox_areas, 'average_area': np.mean(bbox_areas) if bbox_areas else 0}\n",
        "    return features'''\n",
        "'''\n",
        "def extract_features_from_mri(yolo_output):\n",
        "    bbox_areas = []\n",
        "    for det in yolo_output:\n",
        "        # Check if the detection row has enough elements\n",
        "        if len(det) >= 5 and det[4] >= 0.25:  # confidence threshold\n",
        "            bbox_area = (det[2] - det[0]) * (det[3] - det[1])\n",
        "            bbox_areas.append(bbox_area.item())\n",
        "        else:\n",
        "            print(f\"Unexpected detection row structure: {det}\")\n",
        "    features = {'bbox_areas': bbox_areas, 'average_area': np.mean(bbox_areas) if bbox_areas else 0}\n",
        "    return features'''\n",
        "\n",
        "def extract_features_from_cropped_image(cropped_image):\n",
        "    # Logic to extract features from the cropped image segment.\n",
        "    # This could involve various image analysis techniques,\n",
        "    # and it heavily depends on what specific features you're interested in.\n",
        "\n",
        "    # For example, you might want to calculate statistics on pixel values\n",
        "    # (mean, standard deviation), texture analysis, or other relevant metrics.\n",
        "    # The 'cropped_image' is expected to be a 2D array representing grayscale intensity.\n",
        "\n",
        "    # Below is a simplified example where we're just using the mean intensity.\n",
        "    average_intensity = np.mean(cropped_image)\n",
        "    features = {'average_intensity': average_intensity}\n",
        "\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "PnawwAKZnJCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute dynamic threshold based on patient data and extracted features\n",
        "def compute_dynamic_threshold(feature, patient_data, initial_threshold, weights, alpha):\n",
        "    weighted_sum = sum(patient_data[key] * weights[key] for key in weights)\n",
        "    adjusted_threshold = initial_threshold + alpha * feature['average_area'] * weighted_sum  # using average area\n",
        "    return adjusted_threshold"
      ],
      "metadata": {
        "id": "N781vd-anK_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify tumor based on feature and threshold\n",
        "def classify_tumor(feature, threshold):\n",
        "    average_area = feature['average_area']\n",
        "    return \"malignant\" if average_area > threshold else \"benign\""
      ],
      "metadata": {
        "id": "UZ2WbUlbnN4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def yolov5_inference(model, images):\n",
        "    # This function should run the model and return the bounding boxes, classifications, and confidence scores.\n",
        "    # Here, you'll need to replace this with your actual inference code that processes the model's results.\n",
        "    results = model(images, size=640)  # Includes NMS\n",
        "    # Extract information from 'results' and return it\n",
        "    # For example: bounding_boxes, classifications, confidence_scores\n",
        "    # Make sure to replace the line below with your actual extraction logic.\n",
        "    return results.xyxy[0]  # Modify as per your implementation'''\n",
        "\n",
        "def yolov5_inference(model, image):\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Perform inference\n",
        "    results = model([img], size=640)  # Use a list as model expects it\n",
        "\n",
        "    # Parse results to get bounding boxes, classifications, and confidence scores\n",
        "    results_data = results.xyxy[0].numpy()  # Convert to numpy array\n",
        "    bounding_boxes, classifications, confidence_scores = [], [], []\n",
        "\n",
        "    for det in results_data:\n",
        "        bbox = [int(num) for num in det[:4]]  # Extract bounding box coordinates\n",
        "        conf_score = float(det[4])  # Extract confidence score\n",
        "        class_id = int(det[5])  # Extract class ID\n",
        "\n",
        "        # You can translate class_id to actual class name if you have a mapping.\n",
        "        classification = 'Tumor'  # Replace with your class name or mapping\n",
        "\n",
        "        bounding_boxes.append(bbox)\n",
        "        classifications.append(classification)\n",
        "        confidence_scores.append(conf_score)\n",
        "\n",
        "    return bounding_boxes, classifications, confidence_scores\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e6g8VujCnSQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_threshold_adjustment(bounding_box, mri_image, patient_data, initial_threshold, weights, alpha):\n",
        "    # Ensure bounding box coordinates are integers, as they are used for slicing the image tensor.\n",
        "    bounding_box_int = [int(coord) for coord in bounding_box]\n",
        "\n",
        "    # Correctly slice the tensor using bounding box coordinates. Assuming the coordinates are [x1, y1, x2, y2].\n",
        "    x1, y1, x2, y2 = bounding_box_int\n",
        "    cropped_image = mri_image[y1:y2, x1:x2]  # Slicing for a 2D tensor since mri_image seems to be a grayscale image.\n",
        "\n",
        "    # Extract features from the cropped area of the MRI\n",
        "    feature = extract_features_from_mri(cropped_image)  # Note: You might need to adjust the feature extraction function for the cropped image.\n",
        "\n",
        "    # Compute dynamic threshold\n",
        "    dynamic_threshold = compute_dynamic_threshold(feature, patient_data, initial_threshold, weights, alpha)\n",
        "\n",
        "    return dynamic_threshold\n",
        "\n"
      ],
      "metadata": {
        "id": "Vqi-vd0ZnXDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(image_path, patient_data, model_path):\n",
        "    # Load the model\n",
        "    model = load_yolo_model(model_path)\n",
        "\n",
        "    # Perform inference with YOLO\n",
        "    rresults = model([img], size=640)\n",
        "\n",
        "    # Parse results to get bounding boxes, classifications, and confidence scores\n",
        "    results_data = results.xyxy[0].numpy()  # Convert to numpy array\n",
        "    bounding_boxes, classifications, confidence_scores = [], [], []\n",
        "\n",
        "    for det in results_data:\n",
        "        bbox = [int(num) for num in det[:4]]  # Extract bounding box coordinates\n",
        "        conf_score = float(det[4])  # Extract confidence score\n",
        "        class_id = int(det[5])  # Extract class ID\n",
        "\n",
        "        # Here, you might translate class_id to actual class name if you have a mapping.\n",
        "        classification = 'Tumor'  # Replace with your class name or mapping\n",
        "\n",
        "        bounding_boxes.append(bbox)\n",
        "        classifications.append(classification)\n",
        "        confidence_scores.append(conf_score)\n",
        "\n",
        "    # Now, we need to apply the dynamic thresholding to these results\n",
        "    final_detections = []\n",
        "    initial_threshold = 100  # You should adjust this based on your requirements\n",
        "    weights = {\"age\": 0.5, \"sex\": 0.3, \"history\": 0.2}  # Sample weights\n",
        "    alpha = 0.05  # Sample alpha\n",
        "\n",
        "    for bbox, classification, conf_score in zip(bounding_boxes, classifications, confidence_scores):\n",
        "        dynamic_threshold = dynamic_threshold_adjustment(bbox, np.array(img), patient_data, initial_threshold, weights, alpha)  # Pass the actual image data\n",
        "\n",
        "        if conf_score > dynamic_threshold:\n",
        "            final_detections.append((bbox, classification, conf_score))\n",
        "\n",
        "    return final_detections\n",
        "\n",
        "'''\n",
        "def run_inference(image_path, patient_data, model_path):\n",
        "    model = load_yolo_model(model_path)\n",
        "    img = Image.open(image_path)  # single image\n",
        "    imgs = [img]  # batch of images\n",
        "\n",
        "    # Run YOLOv5 inference and get bounding boxes, classifications, and confidence scores\n",
        "    results = yolov5_inference(model, imgs)\n",
        "\n",
        "    # Initial classification (global for the whole image)\n",
        "    mri_features = extract_features_from_mri(results)\n",
        "    initial_threshold = 100  # Arbitrary value, can be adjusted\n",
        "    weights = {\"age\": 0.5, \"sex\": 0.3, \"history\": 0.2}\n",
        "    alpha = 0.05\n",
        "    global_dynamic_threshold = compute_dynamic_threshold(mri_features, patient_data, initial_threshold, weights, alpha)\n",
        "    global_classification = classify_tumor(mri_features, global_dynamic_threshold)\n",
        "\n",
        "    # Dynamic threshold adjustment for each bounding box\n",
        "    final_detections = []\n",
        "    for det in results:  # loop through detections\n",
        "        bbox = det[:4]  # extract bounding box coordinates\n",
        "        conf_score = det[4]  # confidence score\n",
        "        classification = 'malignant' if det[5] == 0 else 'benign'  # just an example; adapt it based on your model's classes\n",
        "\n",
        "        dynamic_threshold = dynamic_threshold_adjustment(bbox, results, patient_data, initial_threshold, weights, alpha)\n",
        "\n",
        "        if conf_score > dynamic_threshold:\n",
        "            final_detections.append((bbox.tolist(), classification, conf_score.item()))  # converting tensor to list\n",
        "\n",
        "\n",
        "    return global_classification, final_detections'''"
      ],
      "metadata": {
        "id": "hQ9L_qmcnd0p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "753f491d-0e34-44d9-c4ac-f92a5ffad99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef run_inference(image_path, patient_data, model_path):\\n    model = load_yolo_model(model_path)\\n    img = Image.open(image_path)  # single image\\n    imgs = [img]  # batch of images\\n\\n    # Run YOLOv5 inference and get bounding boxes, classifications, and confidence scores\\n    results = yolov5_inference(model, imgs)\\n\\n    # Initial classification (global for the whole image)\\n    mri_features = extract_features_from_mri(results)\\n    initial_threshold = 100  # Arbitrary value, can be adjusted\\n    weights = {\"age\": 0.5, \"sex\": 0.3, \"history\": 0.2}\\n    alpha = 0.05\\n    global_dynamic_threshold = compute_dynamic_threshold(mri_features, patient_data, initial_threshold, weights, alpha)\\n    global_classification = classify_tumor(mri_features, global_dynamic_threshold)\\n\\n    # Dynamic threshold adjustment for each bounding box\\n    final_detections = []\\n    for det in results:  # loop through detections\\n        bbox = det[:4]  # extract bounding box coordinates\\n        conf_score = det[4]  # confidence score\\n        classification = \\'malignant\\' if det[5] == 0 else \\'benign\\'  # just an example; adapt it based on your model\\'s classes\\n\\n        dynamic_threshold = dynamic_threshold_adjustment(bbox, results, patient_data, initial_threshold, weights, alpha)\\n\\n        if conf_score > dynamic_threshold:\\n            final_detections.append((bbox.tolist(), classification, conf_score.item()))  # converting tensor to list\\n\\n\\n    return global_classification, final_detections'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''def main():\n",
        "    model_path = '/content/tumor_detector_axial.pt'  # update this path\n",
        "    image_path = '/content/drive/MyDrive/data/axial/images/test/00018_126.jpg'  # update this path\n",
        "    patient_data = {\"age\": 45, \"sex\": 1, \"history\": 1}\n",
        "\n",
        "    global_classification, final_detections = run_inference(image_path, patient_data, model_path)\n",
        "\n",
        "    print(f\"The overall tumor classification is {global_classification}.\")\n",
        "    print(\"Final Detections after Dynamic Threshold Adjustment:\")\n",
        "    for detection in final_detections:\n",
        "        bbox, classification, conf_score = detection\n",
        "        print(f\"Detected: {classification} with confidence {conf_score} at {bbox}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()'''\n",
        "\n",
        "def main():\n",
        "    model_path = '/content/tumor_detector_axial.pt'  # update this path\n",
        "    image_path = '/content/drive/MyDrive/data/axial/images/test/00018_134.jpg'  # update this path\n",
        "    patient_data = {\"age\": 45, \"sex\": 1, \"history\": 1}\n",
        "\n",
        "    model = load_yolo_model(model_path)\n",
        "\n",
        "    # Run YOLOv5 inference\n",
        "    bounding_boxes, classifications, confidence_scores = yolov5_inference(image_path, model)\n",
        "    print(f\"Bounding Boxes: {bounding_boxes}\")\n",
        "    print(f\"Classifications: {classifications}\")\n",
        "    print(f\"Confidence Scores: {confidence_scores}\")\n",
        "\n",
        "    final_detections = run_inference(image_path, patient_data, model_path)\n",
        "    print(\"Final Detections:\", final_detections)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "NfMHfT3jni52",
        "outputId": "737d557e-ec2f-46f0-81c6-33c892da4d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-10-21 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 212 layers, 20856975 parameters, 0 gradients, 47.9 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-fa6b21b6256d>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-fa6b21b6256d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Run YOLOv5 inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mbounding_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolov5_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Bounding Boxes: {bounding_boxes}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Classifications: {classifications}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-44d41388fe5e>\u001b[0m in \u001b[0;36myolov5_inference\u001b[0;34m(model, image)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0myolov5_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Perform inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_yolo_model(model_path):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path, force_reload=True).to(device)\n",
        "    return model\n",
        "\n",
        "def classify_tumor(features, patient_data):\n",
        "    # Implement your classification logic here based on the features and patient data\n",
        "    # This is a placeholder for your classification logic\n",
        "    score = features['average_intensity']  # example feature\n",
        "    if score > 0.5:  # placeholder condition\n",
        "        return 'Malignant'\n",
        "    else:\n",
        "        return 'Benign'\n",
        "\n",
        "def extract_features_from_cropped_image(cropped_image):\n",
        "    # Assuming cropped_image is a numpy array representing the cropped area of the MRI\n",
        "    average_intensity = np.mean(cropped_image)\n",
        "    features = {'average_intensity': average_intensity}\n",
        "    return features\n",
        "\n",
        "def compute_dynamic_threshold(features, patient_data, initial_threshold, weights, alpha):\n",
        "    weighted_sum = sum(patient_data[key] * weights[key] for key in patient_data)\n",
        "    adjusted_threshold = initial_threshold + alpha * features['average_intensity'] * weighted_sum\n",
        "    return adjusted_threshold\n",
        "\n",
        "def run_inference(image_path, patient_data, model_path):\n",
        "    # Load the model\n",
        "    model = load_yolo_model(model_path)\n",
        "\n",
        "    original_image_pil = Image.open(image_path)\n",
        "    original_image = np.array(original_image_pil)\n",
        "\n",
        "    # Perform inference with YOLO\n",
        "    results = model([original_image_pil], size=640)\n",
        "\n",
        "    # Parse results to get bounding boxes, classifications, and confidence scores\n",
        "    results_data = results.xyxy[0].numpy()  # Convert to numpy array\n",
        "    bounding_boxes, classifications, confidence_scores = [], [], []\n",
        "\n",
        "    for det in results_data:\n",
        "        bbox = [int(num) for num in det[:4]]\n",
        "        conf_score = float(det[4])\n",
        "        class_id = int(det[5])\n",
        "        classification = 'Tumor'  # This is a placeholder; replace with actual class name if needed\n",
        "        bounding_boxes.append(bbox)\n",
        "        classifications.append(classification)\n",
        "        confidence_scores.append(conf_score)\n",
        "\n",
        "    # Parameters for dynamic threshold adjustment\n",
        "    initial_threshold = 0.25  # Example value; please adjust as necessary\n",
        "    weights = {\"age\": 0.1, \"sex\": 0.1, \"history\": 0.1}  # Example values; adjust as necessary\n",
        "    alpha = 0.1  # Sensitivity of the adjustment; adjust as necessary\n",
        "\n",
        "    final_detections = []\n",
        "    annotated_images = []\n",
        "\n",
        "    for bbox, classification, conf_score in zip(bounding_boxes, classifications, confidence_scores):\n",
        "        # Assuming that 'results.imgs' contains the original image data in a numpy array format\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        cropped_image = original_image[y1:y2, x1:x2]  # Slicing the image to get the region of interest\n",
        "\n",
        "        features = extract_features_from_cropped_image(cropped_image)\n",
        "\n",
        "        dynamic_threshold = compute_dynamic_threshold(features, patient_data, initial_threshold, weights, alpha)\n",
        "\n",
        "        if conf_score > dynamic_threshold:\n",
        "            # Classify the tumor based on extracted features and patient data\n",
        "            tumor_type = classify_tumor(features, patient_data)\n",
        "            final_detections.append((bbox, tumor_type, conf_score, dynamic_threshold))\n",
        "\n",
        "            # Draw bounding box and add annotation\n",
        "            draw = ImageDraw.Draw(original_image_pil)\n",
        "            draw.rectangle([(x1, y1), (x2, y2)], outline=\"red\", width=2)\n",
        "            draw.text((x1, y1 - 10), f\"{tumor_type} (Confidence: {conf_score:.2f})\", fill=\"red\")\n",
        "\n",
        "    save_directory = \"annotated_images\"\n",
        "    if not os.path.exists(save_directory):\n",
        "        os.makedirs(save_directory)  # Create directory if it does not exist\n",
        "\n",
        "    # Save the annotated image\n",
        "    save_path = os.path.join(save_directory, \"annotated_image.jpg\")  # adjust the path and file name as needed\n",
        "    original_image_pil.save(save_path)\n",
        "    annotated_images.append(save_path)\n",
        "\n",
        "\n",
        "    return final_detections, annotated_images\n",
        "\n",
        "def main():\n",
        "    model_path = '/content/tumor_detector_axial.pt'\n",
        "    image_path = '/content/drive/MyDrive/data/axial/images/test/00018_134.jpg'\n",
        "    patient_data = {\"age\": 45, \"sex\": 1, \"history\": 1}  # Update with actual patient data\n",
        "\n",
        "    final_detections = run_inference(image_path, patient_data, model_path)\n",
        "\n",
        "    print(\"Final Detections:\")\n",
        "    for detection in final_detections:\n",
        "        print(f\"Bounding Box: {detection[0]}, Classification: {detection[1]}, Confidence Score: {detection[2]}, Dynamic Threshold: {detection[3]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "rYhX0bUOYR-A",
        "outputId": "bb545f88-4ad8-4995-c0a8-9101fa74f1d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2023-10-22 Python-3.10.12 torch-2.1.0+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 212 layers, 20856975 parameters, 0 gradients, 47.9 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Detections:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-64f25535a902>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-64f25535a902>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Detections:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdetection\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_detections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Bounding Box: {detection[0]}, Classification: {detection[1]}, Confidence Score: {detection[2]}, Dynamic Threshold: {detection[3]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItiQz23eMJhj",
        "outputId": "8cf7a00f-98d8-484b-8ee7-9c226187f514"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}